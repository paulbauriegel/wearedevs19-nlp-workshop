{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teach a machine to understand human language\n",
    "##### WeAreDeveloper World Congress 2019\n",
    "\n",
    "## Part 2 - Answer a Questions based on Named-Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**: We want to answer to questions that address a specific questions on devices, like *What kind of CPU does my Samsung Galaxy S7 have?*\n",
    "\n",
    "**Step 1**: Mark device names and properties by using Named-Entity Recognition(NER)<br>\n",
    "**Step 2**: Build a query against the Wikidata Ontology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building a NER solution, let's have a look into the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = \"/home/paul/Downloads/classifier_dataset.csv\"\n",
    "df = pd.read_csv(path, encoding=\"utf-8\", header=0, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Like many AUX, the Lombard has S or Lombards</td>\n",
       "      <td>devices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is there still the Monaco 1?</td>\n",
       "      <td>devices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Show me Comfort Pro P 500 devices</td>\n",
       "      <td>devices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>which connection options does the Ergotel s ha...</td>\n",
       "      <td>devices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Show me all the colors for the Actron Card</td>\n",
       "      <td>devices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I&amp;#39;m looking for a garnet 1 phone</td>\n",
       "      <td>devices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Which SIM card is in the Google Pixel?</td>\n",
       "      <td>devices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the purchase price for speedort W724 v</td>\n",
       "      <td>devices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What weighs a Samsung phone?</td>\n",
       "      <td>devices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is the bottom line between the Google Pix...</td>\n",
       "      <td>devices</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question      cat\n",
       "0       Like many AUX, the Lombard has S or Lombards  devices\n",
       "1                       Is there still the Monaco 1?  devices\n",
       "2                  Show me Comfort Pro P 500 devices  devices\n",
       "3  which connection options does the Ergotel s ha...  devices\n",
       "4         Show me all the colors for the Actron Card  devices\n",
       "5               I&#39;m looking for a garnet 1 phone  devices\n",
       "6             Which SIM card is in the Google Pixel?  devices\n",
       "7     What is the purchase price for speedort W724 v  devices\n",
       "8                       What weighs a Samsung phone?  devices\n",
       "9  What is the bottom line between the Google Pix...  devices"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['cat']=='devices'][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mark device names and properties by using Named-Entity Recognition(NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different models for NER:\n",
    "- Conditional Random Fields (CFR)\n",
    "- Convolutional Neural Networks(CNN)\n",
    "- Long Short Term Memory(LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conditional Random Fields (CFR)** <br>\n",
    "<img src=\"file:///home/paul/Documents/wwc-demo/crf.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutional Neural Networks(CNN)** <br>\n",
    "<img src=\"https://user-images.githubusercontent.com/7529838/34460821-5e3542f4-ee5d-11e7-93d4-f8ce81984b89.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Long Short Term Memory(LSTM)**\n",
    "<img src=\"file:///home/paul/Documents/wwc-demo/lstm3.png\" width=\"400\">\n",
    "<img src=\"file:///home/paul/Documents/wwc-demo/lstm1.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try some pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the State-of-the-art Named-Entity Recognition models some kind of LSTM. The training of these models would take more time than we have. So let's try some Named-Entity Recognition models. Some performant and easy-to-use models are available trough modules like flair or allennlp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some models from flair a project by Zalando Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The have trained a BiLSTM-CRF with some special embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-06 14:30:55,632 loading file /home/paul/.flair/models/en-ner-conll03-v0.4.pt\n",
      "2019-06-06 14:31:17,221 loading file /home/paul/.flair/models/en-ner-ontonotes-v0.3.pt\n",
      "2019-06-06 14:31:31,296 loading file /home/paul/Downloads/resources/taggers/example-ner/best-model.pt\n"
     ]
    }
   ],
   "source": [
    "tagger_conll03 = SequenceTagger.load('ner')\n",
    "tagger_ontonotes = SequenceTagger.load('ner-ontonotes')\n",
    "tagger_wikiner = SequenceTagger.load('/home/paul/Downloads/resources/taggers/example-ner/best-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tags(tagger,text):\n",
    "    sentence = Sentence(text)\n",
    "    tagger.predict(sentence)\n",
    "\n",
    "    for entity in sentence.get_spans('ner'):\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PER-span [1,2]: \"George Washington\"\n",
      "LOC-span [5]: \"Washington\"\n"
     ]
    }
   ],
   "source": [
    "print_tags(tagger_conll03, 'George Washington went to Washington .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What kind of CPU has the Samsung Galaxy S7\n",
      "ORG-span [4]: \"CPU\"\n",
      "MISC-span [7,8,9]: \"Samsung Galaxy S7\"\n"
     ]
    }
   ],
   "source": [
    "print('What kind of CPU has the Samsung Galaxy S7')\n",
    "print_tags(tagger_conll03, 'What kind of CPU has the Samsung Galaxy S7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/lib/python3.6/site-packages/allennlp/data/token_indexers/token_characters_indexer.py:55: UserWarning: You are using the default value (0) of `min_padding_length`, which can cause some subtle bugs (more info see https://github.com/allenai/allennlp/issues/1954). Strongly recommend to set a value, usually the maximum size of the convolutional layer size when using CnnEncoder.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/ner-model-2018.12.18.tar.gz\")\n",
    "res = predictor.predict(\n",
    "  sentence=\"What kind of CPU has the Samsung Galaxy S7\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU U-ORG\n",
      "Samsung Galaxy S7 MISC\n"
     ]
    }
   ],
   "source": [
    "misc = []\n",
    "for i, e in enumerate(res['words']):\n",
    "    if res['tags'][i] == 'O':\n",
    "        was_misc = False\n",
    "    elif res['tags'][i].endswith('MISC'):\n",
    "        misc += [e]\n",
    "    else:\n",
    "        print(e, res['tags'][i])\n",
    "print(' '.join(misc), 'MISC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the pretrained models we have, detect the products quite accurate. But not the properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-Training of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_spacy(data,iterations):\n",
    "    TRAIN_DATA = data\n",
    "    nlp = spacy.blank('de')  # create blank Language class\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "       \n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "         for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        nlp.begin_training()\n",
    "        for itn in range(iterations):\n",
    "            print(\"Starting iteration \" + str(itn))\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses)\n",
    "            print(losses)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a new model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('/home/paul/Downloads/result.json') as f:\n",
    "    ner_dataset = json.load(f)\n",
    "\n",
    "prdnlp = train_spacy(ner_dataset, 20)\n",
    "\n",
    "with open('telekom-as.pkl','wb') as outfile:\n",
    "    pickle.dump(prdnlp,outfile)\n",
    "# Save our trained Model\n",
    "modelfile = input(\"Enter your Model Name: \")\n",
    "prdnlp.to_disk(modelfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your testing text: \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.blank('de')\n",
    "prdnlp = spacy.load('/home/paul/Documents/wwc-demo/telekom-test')\n",
    "test_text = input(\"Enter your testing text: \")\n",
    "doc = prdnlp(test_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a query against the Wikidata Ontology\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/Datamodel_in_Wikidata.svg/2560px-Datamodel_in_Wikidata.svg.png\" width=600>\n",
    "Now let's use that knowledge and query against Wikidata for an answer. You can do so by building SPARQL Queries. You can build and try queries on you own using this endpoint: https://query.wikidata.org/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_property(property_name):\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    query = \"\"\"\n",
    "    SELECT ?property ?propertyLabel WHERE {{\n",
    "        ?property rdfs:label ?propertyLabel. \n",
    "        ?property a wikibase:Property .\n",
    "        FILTER(lang(LCASE(?propertyLabel)) = \"de\").\n",
    "        FILTER(CONTAINS(LCASE(?propertyLabel), \"{}\"@de)). \n",
    "    }} LIMIT 1\n",
    "    \"\"\".replace('\\n','').format(property_name.lower())\n",
    "    query =  re.sub(' +', ' ', query)\n",
    "    r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "    data = r.json()\n",
    "    name = data[\"results\"][\"bindings\"][0][\"property\"][\"value\"]\n",
    "    entity_id = name.split('http://www.wikidata.org/entity/')[1]\n",
    "    return entity_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device(device_name, property_id):\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    query = \"\"\"\n",
    "    SELECT ?itemLabel ?propertyLabel\n",
    "    WHERE {{ \n",
    "      ?item rdfs:label ?itemLabel. \n",
    "      ?item wdt:P279 wd:Q22645.\n",
    "      FILTER(CONTAINS(LCASE(?itemLabel), \"{0}\"@de)). \n",
    "      ?item wdt:{1} ?property.\n",
    "      ?property rdfs:label ?propertyLabel. \n",
    "      FILTER(LANG(?propertyLabel) = \"de\"). \n",
    "    }} LIMIT 1\n",
    "    \"\"\".replace('\\n','').format(device_name.lower(), property_id)\n",
    "    query =  re.sub(' +', ' ', query)\n",
    "    r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "    data = r.json()\n",
    "    name = data[\"results\"]['bindings']\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'itemLabel': {'xml:lang': 'de',\n",
       "   'type': 'literal',\n",
       "   'value': 'Samsung Galaxy S7'},\n",
       "  'propertyLabel': {'xml:lang': 'de',\n",
       "   'type': 'literal',\n",
       "   'value': 'Samsung Exynos'}}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_device(\"Galaxy S7\", get_property(\"CPU\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use all that to answer simple questions\n",
    "Like *What kind of CPU does the Samsung Galaxy S7 have?*. Keep in mind that you can get results for properties that are defined in the Wikidata Entry. Additionally this query builder is not meant to answer complex questions, because we are missing a lot of logic therefore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your testing text: blabbla\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-dca7ee700aa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your testing text: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprdnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mproduct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'PRODUKT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpropert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'EIGENSCHAFT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_property\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpropert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "test_text = input(\"Enter your testing text: \")\n",
    "doc = prdnlp(test_text)\n",
    "product = [ent.text for ent in doc.ents if ent.label_ == 'PRODUKT'][0]\n",
    "propert = [ent.text for ent in doc.ents if ent.label_ == 'EIGENSCHAFT'][0]\n",
    "res = get_device(product, get_property(propert))\n",
    "print('The ' + product + ' has a ' + res[0]['propertyLabel']['value'] + ' ' + propert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
